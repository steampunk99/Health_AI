{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.llms import CTransformers\n",
    "from langchain.chains import RetrievalQA\n",
    "import chainlit as cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_FAISS_PATH = \"vectorstores/db_faiss\"\n",
    "import openai\n",
    "import re\n",
    "import streamlit as st\n",
    "from prompts import get_system_prompt\n",
    "\n",
    "st.title(\"☃️ Frosty\")\n",
    "\n",
    "# Initialize the chat messages history\n",
    "openai.api_key = \"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\"\n",
    "if \"messages\" not in st.session_state:\n",
    "    # system prompt includes table information, rules, and prompts the LLM to produce\n",
    "    # a welcome message to the user.\n",
    "    st.session_state.messages = [{\"role\": \"system\", \"content\": get_system_prompt()}]\n",
    "\n",
    "# Prompt for user input and save\n",
    "if prompt := st.chat_input():\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "# display the existing chat messages\n",
    "for message in st.session_state.messages:\n",
    "    if message[\"role\"] == \"system\":\n",
    "        continue\n",
    "    with st.chat_message(message[\"role\"]):\n",
    "        st.write(message[\"content\"])\n",
    "        if \"results\" in message:\n",
    "            st.dataframe(message[\"results\"])\n",
    "\n",
    "# If last message is not from assistant, we need to generate a new response\n",
    "if st.session_state.messages[-1][\"role\"] != \"assistant\":\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        response = \"\"\n",
    "        resp_container = st.empty()\n",
    "        for delta in openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": m[\"role\"], \"content\": m[\"content\"]} for m in st.session_state.messages],\n",
    "            stream=True,\n",
    "        ):\n",
    "            response += delta.choices[0].delta.get(\"content\", \"\")\n",
    "            resp_container.markdown(response)\n",
    "\n",
    "        message = {\"role\": \"assistant\", \"content\": response}\n",
    "        # Parse the response for a SQL query and execute if available\n",
    "        sql_match = re.search(r\"```sql\\n(.*)\\n```\", response, re.DOTALL)\n",
    "        if sql_match:\n",
    "            sql = sql_match.group(1)\n",
    "            conn = st.experimental_connection(\"snowpark\")\n",
    "            message[\"results\"] = conn.query(sql)\n",
    "            st.dataframe(message[\"results\"])\n",
    "        st.session_state.messages.append(message) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
